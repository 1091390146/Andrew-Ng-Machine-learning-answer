# Andrew-Ng Machinelearning answer  

  ex1、目前只添加了第六节线性回归的答案，单变量和多变量完整答案，后续会继续添加;  

  ex2、添加了逻辑回归完整答案，本章重在理解，从推导sigmoid函数到极大似然估计法推导公式，再求导，运用梯度上升或者*(-1/m)运用梯度下降；
正则化需要注意是用来限制theta的，而theta(1)也就是对应X为1的那一列不需要正则化，因为会限制theta(1)上升或者下降，无法达到最值点;  
  
  ex3、完成了第三次多元分类与神经网络作业，注意理解多元分类运用的逻辑回归，本质也是一个0 1 分类问题，只不过把相对应的单元划为1，其余划为0去训练;
oneVsAll.m文件里面有对本次作业图片识别的对应说明，注意附加的神经网络作业已经给出了Theta抛砖引玉，只需理解即可;  
  
 ex4、完成了神经网络BP算法作业，这一章理解起来很难，前一张多元分类是依次训练10组数据，对应0~9,而我们需要手动依次相乘
会得到10组数据去进行判断是哪一个数字，而神经网络图片数字识别是直接训练10个输出，多元分类的theta对应的的是每个图片数字的最优解，
而神经网络不是，它是由每个单元共同决定输出结果，层层联系，最后输出十个输出单元去判断数字，
至于神经网络公式的推导为下面这个网址: https://blog.csdn.net/qq_32865355/article/details/80260212  
    但是文中的代价函数为线性方程的最小二乘法的代价,即E = 1/2/m*(y-o)^2,得到的公式如下所示:  
![链接内的代价函数推导的误差项](https://s2.ax1x.com/2019/07/18/ZX7ixU.jpg)  
然而作业中用的代价公式是sigmoid的:![作业误差项说明](C:\Users\Think\Desktop\IMG20190718164838.jpg)  
最后得到sigmoid代价函数推导的公式应该为:![公式:](C:\Users\Think\Desktop\2018050922212487.jpg)


